{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZyvXPKFrvbSY"
   },
   "source": [
    "Trước đây bạn đã huấn luyện Mạng nơ-ron 2 lớp (với một lớp ẩn duy nhất). Trong lab này, bạn sẽ xây dựng một mạng nơ-ron sâu với bao nhiêu lớp tùy thích!\n",
    "\n",
    "- Trong notebook này, bạn sẽ thực hiện tất cả các hàm cần thiết để xây dựng một mạng nơ-ron sâu.\n",
    "- Trong lab tiếp theo, bạn sẽ sử dụng các hàm này để xây dựng một mạng nơ-ron sâu cho phân loại ảnh.\n",
    "\n",
    "**Sau lab này, bạn sẽ có thể:**\n",
    "- Sử dụng các đơn vị phi tuyến tính như ReLU để cải thiện mô hình của bạn\n",
    "- Xây dựng mạng nơ-ron sâu hơn (với nhiều hơn 1 lớp ẩn)\n",
    "- Triển khai một lớp mạng nơ-ron dễ sử dụng\n",
    "\n",
    "**Kí hiệu**:\n",
    "- Chỉ số trên $[l]$ biểu thị số lượng được liên kết với lớp $l^{th}$. \n",
    "    - Ví dụ: $a^{[L]}$ là kích hoạt lớp $L^{th}$. $W^{[L]}$ và $b^{[L]}$ là các tham số lớp $L^{th}$.\n",
    "- Chỉ số trên $(i)$ biểu thị số lượng được liên kết với ví dụ $i^{th}$. \n",
    "    - Ví dụ: $x^{(i)}$ là ví dụ huấn luyện $i^{th}$.\n",
    "- Chỉ số dưới $i$ biểu thị mục nhập $i^{th}$ của một vectơ.\n",
    "    - Ví dụ: $a^{[l]}_i$ biểu thị mục nhập $i^{th}$ của các lần kích hoạt lớp $l^{th}$.\n",
    "\n",
    "Hãy bắt đầu!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O93yZkZovbSg"
   },
   "source": [
    "## 1 - Thư viện\n",
    "\n",
    "Đầu tiên, hãy nhập tất cả các thư viện mà chúng ta sẽ cần trong lab này.\n",
    "- [numpy](www.numpy.org) là thư viện chính cho tính toán khoa học trong Python.\n",
    "- [matplotlib](http://matplotlib.org) là một thư viện để vẽ đồ thị trong Python.\n",
    "- dnn_utils cung cấp một số hàm cần thiết cho notebook này.\n",
    "- testCases cung cấp một số trường hợp thử nghiệm để đánh giá tính đúng đắn của các hàm\n",
    "- np.random.seed(1) được sử dụng để giữ cho tất cả các lệnh gọi hàm ngẫu nhiên nhất quán. Nó sẽ giúp chúng ta đánh giá công việc. Vui lòng không thay đổi seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IX8P6c1KvbSh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases_v3 import *\n",
    "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # thiết lập kích thước mặc định của biểu đồ\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "gsdZboVGvbSi"
   },
   "source": [
    "## 2 - Sơ lược về Lab\n",
    "\n",
    "Để xây dựng mạng nơ-ron của mình, chúng ta sẽ thực hiện một số \"helper function\" (hàm trợ giúp). Các hàm trợ giúp này sẽ được sử dụng trong lab tiếp theo để xây dựng mạng nơ-ron hai lớp và mạng nơ-ron L-lớp. Mỗi hàm trợ giúp nhỏ mà chúng ta thực hiện sẽ có hướng dẫn chi tiết hướng dẫn qua các bước cần thiết. Đây là bản sơ lược của lab này, chúng ta sẽ:\n",
    "\n",
    "- Khởi tạo các tham số cho mạng hai lớp và cho mạng nơ-ron $ L $ -lớp. \n",
    "- Thực hiện mô-đun lan truyền xuôi (được hiển thị bằng màu tím trong hình bên dưới).\n",
    "     - Hoàn thành phần TUYẾN TÍNH của bước lan truyền xuôi của một lớp (kết quả là $ Z ^ {[l]} $).\n",
    "     - Chúng tôi cung cấp cho bạn hàm KÍCH HOẠT (relu/sigmoid).\n",
    "     - Kết hợp hai bước trước đó thành một hàm forward [LINEAR->ACTIVATION] mới.\n",
    "     - Xếp chồng hàm chuyển tiếp [LINEAR->RELU] L-1 lần (cho lớp 1 đến lớp L-1) và thêm [LINEAR->SIGMOID] vào cuối (cho lớp cuối cùng $ L $). Điều này cung cấp cho bạn hàm L_model_osystem mới.\n",
    "- Tính loss (mất mát).\n",
    "- Thực hiện mô-đun lan truyền ngược (ký hiệu màu đỏ trong hình bên dưới).\n",
    "    - Hoàn thành phần LINEAR của bước lan truyền ngược của một lớp.\n",
    "    - Chúng tôi cung cấp cho bạn gradient của hàm ACTIVATE (relu_backward/sigmoid_backward)\n",
    "    - Combine the previous two steps into a new [LINEAR->ACTIVATION] backward function. Kết hợp hai bước trước đó thành hàm backward [LINEAR->ACTIVATION] mới.\n",
    "    - Xếp chồng [LINEAR->RELU] lùi L-1 lần và thêm [LINEAR->SIGMOID] lùi vào hàm L_model_backward mới\n",
    "- Cuối cùng là cập nhật các tham số.\n",
    "\n",
    "<img src=\"images/final outline.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><center> **Figure 1**</center></caption><br>\n",
    "\n",
    "\n",
    "**Lưu ý** rằng đối với mỗi hàm forward, có một hàm backward tương ứng. Đó là lý do tại sao ở mỗi bước của mô-đun chuyển tiếp, chúng ta sẽ lưu trữ một số giá trị trong cache. Các giá trị được lưu trong cache rất hữu ích cho việc tính toán gradient. Sau đó, trong mô-đun backpropagation, chúng ta sẽ sử dụng cache để tính toán các gradient. Lab này sẽ chỉ cho bạn chính xác cách thực hiện từng bước này."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "UYUNQHGqvbSk"
   },
   "source": [
    "## 3 - Khởi tạo\n",
    "\n",
    "Chúng ta sẽ viết hai hàm trợ giúp khởi tạo các tham số cho mô hình của mình. Hàm đầu tiên sẽ được sử dụng để khởi tạo các tham số cho mô hình 2 lớp. Hàm thứ hai sẽ tổng quát quá trình khởi tạo này thành $ L $ lớp.\n",
    "\n",
    "### 3.1 - Mạng nơ-ron 2 lớp\n",
    "\n",
    "**Task 1**: Tạo và khởi tạo các tham số của mạng nơ-ron 2 lớp.\n",
    "\n",
    "**Hướng dẫn**:\n",
    "- Cấu trúc của mô hình là: *LINEAR -> RELU -> LINEAR -> SIGMOID*.\n",
    "-Sử dụng khởi tạo ngẫu nhiên cho các ma trận trọng số. Sử dụng `np.random.randn(shape)*0.01` với shape chính xác.\n",
    "- Sử dụng khởi tạo bằng không cho các độ lệch. Sử dụng `np.zeros(shape)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Zw0QFtnvvbSk"
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Đối số:\n",
    "    n_x -- kích thước lớp đầu vào\n",
    "    n_h -- kích thước lớp ẩn\n",
    "    n_y -- kích thước lớp đầu ra\n",
    "    \n",
    "    Trả về:\n",
    "    parameters -- dictionary của python chứa các tham số:\n",
    "                    W1 -- ma trận trọng số có shape (n_h, n_x)\n",
    "                    b1 -- vectơ bias có shape (n_h, 1)\n",
    "                    W2 -- ma trận trọng số có shape (n_y, n_h)\n",
    "                    b2 -- vectơ bias có shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    ### BẮT ĐẦU CODE Ở ĐÂY ### (≈ 4 dòng code)\n",
    "    W1 = np.random.randn(n_h,n_x)*0.01\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y,n_h)*0.01\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    ### KẾT THÚC CODE Ở ĐÂY ###\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Zyxltq0zvbSm",
    "outputId": "92a76cf6-e716-4af1-bf2d-87f774763821",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01624345 -0.00611756 -0.00528172]\n",
      " [-0.01072969  0.00865408 -0.02301539]]\n",
      "b1 = [[0.]\n",
      " [0.]]\n",
      "W2 = [[ 0.01744812 -0.00761207]]\n",
      "b2 = [[0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(3,2,1)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dl0OMiQsvbSn"
   },
   "source": [
    "**Kỳ vọng đầu ra**:\n",
    "       \n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td> **W1** </td>\n",
    "    <td> [[ 0.01624345 -0.00611756 -0.00528172]\n",
    " [-0.01072969  0.00865408 -0.02301539]] </td> \n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td> **b1**</td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**W2**</td>\n",
    "    <td> [[ 0.01744812 -0.00761207]]</td>\n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td> **b2** </td>\n",
    "    <td> [[ 0.]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "QancFrP6vbSn"
   },
   "source": [
    "### 3.2 - Mạng nơ-ron L lớp\n",
    "\n",
    "Việc khởi tạo cho một mạng nơ-ron L lớp sâu hơn phức tạp hơn vì có nhiều ma trận trọng số và vectơ bias hơn. Khi hoàn thành `initialize_parameters_deep`, bạn nên đảm bảo rằng kích thước của bạn khớp với mỗi lớp. Nhớ lại rằng $ n ^ {[l]} $ là số đơn vị trong lớp $ l $. Vì vậy, ví dụ: nếu kích thước của đầu vào $ X $ của chúng ta là $ (12288, 209) $ (với $ m = 209 $ chẳng hạn) thì:\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "\n",
    "\n",
    "    <tr>\n",
    "        <td>  </td> \n",
    "        <td> **Shape of W** </td> \n",
    "        <td> **Shape of b**  </td> \n",
    "        <td> **Activation** </td>\n",
    "        <td> **Shape of Activation** </td> \n",
    "    <tr>\n",
    "    \n",
    "    <tr>\n",
    "        <td> **Layer 1** </td> \n",
    "        <td> $(n^{[1]},12288)$ </td> \n",
    "        <td> $(n^{[1]},1)$ </td> \n",
    "        <td> $Z^{[1]} = W^{[1]}  X + b^{[1]} $ </td> \n",
    "        \n",
    "        <td> $(n^{[1]},209)$ </td> \n",
    "    <tr>\n",
    "    \n",
    "    <tr>\n",
    "        <td> **Layer 2** </td> \n",
    "        <td> $(n^{[2]}, n^{[1]})$  </td> \n",
    "        <td> $(n^{[2]},1)$ </td> \n",
    "        <td>$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$ </td> \n",
    "        <td> $(n^{[2]}, 209)$ </td> \n",
    "    <tr>\n",
    "   \n",
    "       <tr>\n",
    "        <td> $\\vdots$ </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$</td> \n",
    "        <td> $\\vdots$  </td> \n",
    "    <tr>\n",
    "    \n",
    "   <tr>\n",
    "        <td> **Layer L-1** </td> \n",
    "        <td> $(n^{[L-1]}, n^{[L-2]})$ </td> \n",
    "        <td> $(n^{[L-1]}, 1)$  </td> \n",
    "        <td>$Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ </td> \n",
    "        <td> $(n^{[L-1]}, 209)$ </td> \n",
    "    <tr>\n",
    "    \n",
    "    \n",
    "   <tr>\n",
    "        <td> **Layer L** </td> \n",
    "        <td> $(n^{[L]}, n^{[L-1]})$ </td> \n",
    "        <td> $(n^{[L]}, 1)$ </td>\n",
    "        <td> $Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}$</td>\n",
    "        <td> $(n^{[L]}, 209)$  </td> \n",
    "    <tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "Hãy nhớ rằng khi chúng ta tính $ W X + b $ trong python, nó sẽ thực hiện broadcasting (truyền phát). Ví dụ, nếu:\n",
    "\n",
    "$$ W = \\begin{bmatrix}\n",
    "    j  & k  & l\\\\\n",
    "    m  & n & o \\\\\n",
    "    p  & q & r \n",
    "\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n",
    "    a  & b  & c\\\\\n",
    "    d  & e & f \\\\\n",
    "    g  & h & i \n",
    "\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n",
    "    s  \\\\\n",
    "    t  \\\\\n",
    "    u\n",
    "\\end{bmatrix}\\tag{2}$$\n",
    "\n",
    "Thì $WX + b$ sẽ là:\n",
    "\n",
    "$$ WX + b = \\begin{bmatrix}\n",
    "    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n",
    "    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n",
    "    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n",
    "\\end{bmatrix}\\tag{3}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCf_Ny0VvbSo"
   },
   "source": [
    "**Task 2**: Thực hiện khởi tạo cho Mạng nơ-ron L lớp.\n",
    "\n",
    "**Hướng dẫn**:\n",
    "- Cấu trúc của mô hình là *[LINEAR->RELU]$\\times $(L-1) -> LINEAR -> SIGMOID *. Tức là, nó có $ L-1 $ lớp sử dụng hàm kích hoạt ReLU, tiếp theo là lớp đầu ra có hàm kích hoạt sigmoid.\n",
    "- Sử dụng khởi tạo ngẫu nhiên cho các ma trận trọng số. Sử dụng `np.random.rand(shape)*0.01`.\n",
    "- Sử dụng khởi tạo thành 0 cho các bias. Sử dụng `np.zeros(shape)`.\n",
    "- Chúng ta sẽ lưu trữ $ n ^ {[l]} $, số lượng đơn vị trong các lớp khác nhau trong biến `layer_dims`. Ví dụ: `layer_dims` cho \"Mô hình phân loại dữ liệu phẳng\" từ lab trước sẽ là [2,4,1]: Có hai đầu vào, một lớp ẩn với 4 đơn vị ẩn và một lớp đầu ra có 1 đơn vị đầu ra . Như vậy có nghĩa là shape của `W1` là (4,2), `b1` là (4,1), `W2` là (1,4) và `b2` là (1,1). Bây giờ bạn sẽ tổng quát điều này thành $ L $ lớp!\n",
    "- Đây là cách thực hiện cho $ L = 1 $ (mạng nơ-ron 1 lớp). Nó sẽ truyền cảm hứng cho bạn để triển khai trường hợp chung (mạng nơ-ron L lớp).\n",
    "```python\n",
    "    if L == 1:\n",
    "        parameters[\"W\" + str(L)] = np.random.randn(layer_dims[1], layer_dims[0]) * 0.01\n",
    "        parameters[\"b\" + str(L)] = np.zeros((layer_dims[1], 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nWPKrD9cvbSo"
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Đối số:\n",
    "    layer_dims -- mảng (list) của python chứa các chiều của từng lớp trong mạng\n",
    "    \n",
    "    Trả về:\n",
    "    parameters -- dictionary của python chứa các tham số \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- ma trận trọng số có shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- vectơ bias có shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # số lớp trong mạng\n",
    "\n",
    "    for l in range(1, L):\n",
    "        ### BẮT ĐẦU CODE Ở ĐÂY ### (≈ 2 dòng code)\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        ### KẾT THÚC CODE Ở ĐÂY ###\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1AejzVY5vbSo",
    "outputId": "658f77fe-bffb-43f4-ef15-768d26d30b79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
      " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
      " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
      " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
      " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
      " [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNCZlnSrvbSp"
   },
   "source": [
    "**Kỳ vọng đầu ra**:\n",
    "       \n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td> **W1** </td>\n",
    "    <td>[[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
    " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
    " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
    " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**b1** </td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**W2** </td>\n",
    "    <td>[[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
    " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
    " [-0.00768836 -0.00230031  0.00745056  0.01976111]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**b2** </td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7iYi_IAvbSp"
   },
   "source": [
    "## 4 - Mô-đun lan truyền xuôi\n",
    "\n",
    "### 4.1 - Truyền xuôi tuyến tính (Linear Forward)\n",
    "Giờ bạn đã khởi tạo các tham số của mình, hãy thực hiện mô-đun lan truyền xuôi. Bạn sẽ bắt đầu bằng cách thực hiện một số hàm cơ bản mà bạn sẽ sử dụng sau này khi thực hiện mô hình. Hãy hoàn thành 3 hàm theo thứ tự sau:\n",
    "\n",
    "- LINEAR\n",
    "- LINEAR -> ACTIVATION trong đó ACTIVATION hoặc là ReLU hoặc Sigmoid. \n",
    "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID (toàn mô hình)\n",
    "\n",
    "Mô-đun truyền xuôi tuyến tính (được vector hóa trên tất cả các ví dụ) tính các phương trình sau:\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n",
    "\n",
    "trong đó $A^{[0]} = X$. \n",
    "\n",
    "**Task 3**: Xây dựng phần tuyến tính của lan truyền xuôi.\n",
    "\n",
    "**Nhắc nhở**:\n",
    "Biểu diễn toán học của đơn vị này là $Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$. Bạn cũng có thể thấy `np.dot()` hữu ích. Nếu chiều không khớp, việc in `W.shape` có thể hữu ích."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "473mMh95vbSq"
   },
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Triển khai phần truyền xuôi tuyến tính của một lớp.\n",
    "\n",
    "    Đối số:\n",
    "    A -- các kích hoạt từ lớp trước đó (hoặc dữ liệu đầu vào): (kích thước lớp trước đó, số ví dụ)\n",
    "    W -- ma trận trọng số: mảng numpy có shape (kích thước lớp hiện tại, kích thước lớp trước đó)\n",
    "    b -- vectơ bias, mảng numpy có shape (kích thước lớp hiện tại, 1)\n",
    "\n",
    "    Trả về:\n",
    "    Z -- đầu vào của hàm kích hoạt, cũng gọi là tham số tiền kích hoạt\n",
    "    cache -- dictionary của python chứa  \"A\", \"W\" và \"b\" ; được lưu trữ để tính toán truyền ngược hiệu quả\n",
    "    \"\"\"\n",
    "    \n",
    "    ### BẮT ĐẦU CODE Ở ĐÂY ### (≈ 1 dòng code)\n",
    "    Z = np.dot(W,A)+b\n",
    "    ### KẾT THÚC CODE Ở ĐÂY ###\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mh7YO590vbSq",
    "outputId": "8ca4a6f0-2c8d-4243-fbc6-301c00ddd297"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[ 3.26295337 -1.23429987]]\n"
     ]
    }
   ],
   "source": [
    "A, W, b = linear_forward_test_case()\n",
    "\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XU2L6pofvbSr"
   },
   "source": [
    "**Kỳ vọng đầu ra**:\n",
    "\n",
    "<table style=\"width:35%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td> **Z** </td>\n",
    "    <td> [[ 3.26295337 -1.23429987]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PLNDTXAvbSr"
   },
   "source": [
    "### 4.2 - Truyền xuôi kích hoạt tuyến tính (Linear-Activation Forward)\n",
    "\n",
    "Trong notebook này, chúng ta sẽ sử dụng 2 hàm kích hoạt:\n",
    "\n",
    "- **Sigmoid**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$. Chúng tôi đã cung cấp cho bạn hàm `sigmoid`. Hàm này trả về **hai** mục: giá trị kích hoạt \"`a`\" và \"`cache`\" có chứa \"`Z` \"(đó là những gì chúng ta sẽ đưa vào hàm truyền ngược tương ứng). Để sử dụng nó, bạn chỉ cần gọi:\n",
    "``` python\n",
    "A, activation_cache = sigmoid(Z)\n",
    "```\n",
    "\n",
    "- **ReLU**: Công thức toán học cho ReLu là $A = RELU(Z) = max(0, Z)$. Chúng tôi đã cung cấp cho bạn hàm `relu`. Hàm này trả về **hai** mục: giá trị kích hoạt \"`A` \" và \" `cache`\" có chứa \"`Z` \"(đó là những gì chúng ta sẽ đưa vào hàm truyền ngược tương ứng). Để sử dụng nó, bạn chỉ cần gọi:\n",
    "``` python\n",
    "A, activation_cache = relu(Z)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJfHEXgxvbSr"
   },
   "source": [
    "Để thuận tiện hơn, chúng ta sẽ nhóm hai hàm (Tuyến tính và Kích hoạt) thành một hàm (TUYẾN TÍNH->KÍCH HOẠT). Do đó, chúng ta sẽ triển khai một hàm thực hiện bước truyền xuôi TUYẾN TÍNH, theo sau là bước truyền xuôi KÍCH HOẠT.\n",
    "\n",
    "**Task 4**: Thực hiện truyền xuôi của lớp *LINEAR->ACTIVATION*. Quan hệ toán học là: $A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$ trong đó kích hoạt \"g\" có thể là sigmoid() hoặc relu(). Sử dụng linear_forward() và hàm kích hoạt chính xác."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Vho4_YcevbSs"
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Triển khai truyền xuôi cho lớp LINEAR->ACTIVATION\n",
    "\n",
    "    Đối số:\n",
    "    A_prev -- các kích hoạt từ lớp trước đó (hoặc dữ liệu đầu vào): (kích thước lớp trước đó, số ví dụ)\n",
    "    W -- ma trận trọng số: mảng numpy có shape (kích thước lớp hiện tại, kích thước lớp trước đó)\n",
    "    b -- vectơ bias, mảng numpy có shape (kích thước lớp hiện tại, 1)\n",
    "    activation -- hàm kích hoạt dùng trong lớp này, được lưu trữ thành xâu văn bản: \"sigmoid\" hoặc \"relu\"\n",
    "\n",
    "    Trả về:\n",
    "    A -- đầu ra của hàm kích hoạt, cũng gọi là giá trị hậu kích hoạt\n",
    "    cache -- dictionary của python chứa \"linear_cache\" và \"activation_cache\";\n",
    "             được lưu trữ để tính toán truyền ngược hiệu quả\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Đầu vào: \"A_prev, W, b\". Đầu ra: \"A, activation_cache\".\n",
    "        ### BẮT ĐẦU CODE Ở ĐÂY ### (≈ 2 dòng code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        ### KẾT THÚC CODE Ở ĐÂY ###\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Đầu vào: \"A_prev, W, b\". Đầu ra: \"A, activation_cache\".\n",
    "        ### BẮT ĐẦU CODE Ở ĐÂY ### (≈ 2 dòng code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        ### KẾT THÚC CODE Ở ĐÂY ###\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "WPldG35MvbSs",
    "outputId": "dd9f9da5-df8c-48c3-e8c5-ad750072a1a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A = [[0.96890023 0.11013289]]\n",
      "With ReLU: A = [[3.43896131 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xln79xfLvbSs"
   },
   "source": [
    "**Kỳ vọng đầu ra**:\n",
    "       \n",
    "<table style=\"width:35%\">\n",
    "  <tr>\n",
    "    <td> **With sigmoid: A ** </td>\n",
    "    <td > [[ 0.96890023  0.11013289]]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> **With ReLU: A ** </td>\n",
    "    <td > [[ 3.43896131  0.        ]]</td> \n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDH0T-NXvbSt"
   },
   "source": [
    "**Lưu ý**: Trong deep learning, phép tính \"[LINEAR-> ACTIVATION]\" được tính là một lớp trong mạng nơ-ron chứ không phải 2 lớp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lmn0OUKjvbSt"
   },
   "source": [
    "### d) Mô hình L-lớp\n",
    "\n",
    "Để thuận tiện hơn nữa khi triển khai Mạng nơ-ron $ L $-lớp, bạn sẽ cần một hàm sao chép cái trước đó (`linear_activation_osystem` với RELU) $ L-1 $ lần, sau đó sử dụng `linear_activation_forward` với SIGMOID.\n",
    "\n",
    "<img src=\"images/model_architecture_kiank.png\" style=\"width:600px;height:300px;\">\n",
    "<caption><center> **Hình 2** : mô hình *[LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID* </center></caption><br>\n",
    "\n",
    "**Task 5**: Thực hiện lan truyền xuôi của mô hình trên.\n",
    "\n",
    "**Hướng dẫn**: Trong đoạn code dưới đây, biến `AL` sẽ biểu thị $A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$. (Điều này đôi khi còn được gọi là `Y mũ`, tức là $\\hat{Y}$.) \n",
    "\n",
    "**Lời khuyên**:\n",
    "- Sử dụng các hàm bạn đã viết trước đó\n",
    "- Sử dụng vòng lặp for để sao chép [LINEAR-> RELU] (L-1) lần\n",
    "- Đừng quên theo dõi các cache trong list \"caches\". Để thêm giá trị mới `c` vào `list`, bạn có thể sử dụng `list.append(c)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YjCDprE7vbSt"
   },
   "outputs": [],
   "source": [
    "# HÀM PHÂN BẬC: L_model_forward\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Triển khai truyền xuôi cho tính toán [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID\n",
    "    \n",
    "    Đối số:\n",
    "    X -- dữ liệu, mảng numpy có shape (kích thước đầu vào, số ví dụ)\n",
    "    parameters -- đầu ra của initialize_parameters_deep()\n",
    "    \n",
    "    Trả về:\n",
    "    AL -- giá trị hậu kích hoạt trước\n",
    "    caches -- danh sách caches có chứa:\n",
    "                mọi cache của linear_relu_forward() (có L-1 trong số đó, được lập chỉ mục từ 0 tới L-2)\n",
    "               cache của linear_sigmoid_forward() (chỉ có một, được lập chỉ mục L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # số lớp trong mạng nơ-ron\n",
    "    \n",
    "    # Triển khai [LINEAR -> RELU]*(L-1). Thêm \"cache\" vào danh sách \"caches\".\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        ### BẮT ĐẦU CODE Ở ĐÂY ### (≈ 2 dòng code)\n",
    "        W=parameters['W' + str(l)]\n",
    "        b=parameters['b' + str(l)]\n",
    "        A, cache = linear_activation_forward(A_prev, W, b, \"relu\")\n",
    "        caches.append(cache)\n",
    "        \n",
    "        ### KẾT THÚC CODE Ở ĐÂY ###\n",
    "    \n",
    "    # Triển khai LINEAR -> SIGMOID. Thêm \"cache\" vào danh sách \"caches\".\n",
    "    ### BẮT ĐẦU CODE Ở ĐÂY ### (≈ 2 dòng code)\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "\n",
    "    ### KẾT THÚC CODE Ở ĐÂY ###\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gt15GplqvbSt",
    "outputId": "0fd8fcb7-5558-4467-8205-25071429c28b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[0.03921668 0.70498921 0.19734387 0.04728177]]\n",
      "Length of caches list = 3\n"
     ]
    }
   ],
   "source": [
    "X, parameters = L_model_forward_test_case_2hidden()\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igZPg2_GvbSt"
   },
   "source": [
    "<table style=\"width:50%\">\n",
    "  <tr>\n",
    "    <td> **AL** </td>\n",
    "    <td > [[ 0.03921668  0.70498921  0.19734387  0.04728177]]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> **Length of caches list ** </td>\n",
    "    <td > 3 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5N2WAoEmvbSu"
   },
   "source": [
    "Tuyệt! Bây giờ bạn có một lan truyền xuôi đầy đủ lấy đầu vào X và xuất ra một vectơ hàng  $A^{[L]}$ chứa các dự đoán. Nó cũng ghi lại tất cả các giá trị trung gian trong \"caches\". Với $A^{[L]}$, chúng ta có thể tính toán chi phí (cost) của dự đoán."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lq-OIjpxvbSu"
   },
   "source": [
    "## 5 - Hàm chi phí (Cost function)\n",
    "\n",
    "Bây giờ bạn sẽ thực hiện lan truyền xuôi và truyền ngược. Chúng ta cần tính toán chi phí để kiểm tra xem mô hình có thực sự đang học hay không.\n",
    "\n",
    "**Task 6**: Tính cross-entropy cost $J$ sử dụng công thức sau: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nsrn7hKDvbSu"
   },
   "outputs": [],
   "source": [
    "# HÀM PHÂN BẬC: compute_cost\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Triển khai hàm chi phí được xác định bởi phương trình (7).\n",
    "\n",
    "    Đối số:\n",
    "    AL -- vectơ xác suất tưng ứng với nhãn dự đoán, shape (1, số ví dụ)\n",
    "    Y -- vectơ true \"label\" (ví dụ: chứa 0 nếu không phải mèo, 1 nếu là mèo), shape (1, số ví dụ)\n",
    "\n",
    "    Trả về:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Tính mất mát từ aL và y.\n",
    "    ### BẮT ĐẦU CODE Ở ĐÂY ### (≈ 1 dòng code)\n",
    "    cost = -(1/m)*np.sum(np.multiply(Y,np.log(AL))+np.multiply(1-Y,np.log(1-AL)))\n",
    "    ### KẾT THÚC CODE Ở ĐÂY ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # Để đảm bảo hình dạng của chi phí như những gì chúng ta mong đợi (ví dụ: biến [[17]] thành 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ZWXj-xHvbSu",
    "outputId": "0ccf5980-b8ad-4f5b-cba9-e2db94a5eb1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.41493159961539694\n"
     ]
    }
   ],
   "source": [
    "Y, AL = compute_cost_test_case()\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(AL, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONZa7S1JvbSu"
   },
   "source": [
    "**Kỳ vọng đầu ra**:\n",
    "\n",
    "<table>\n",
    "\n",
    "    <tr>\n",
    "    <td>**cost** </td>\n",
    "    <td> 0.41493159961539694</td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXyXkmpfvbSv"
   },
   "source": [
    "## 6 - Mô-đun lan truyền ngược\n",
    "\n",
    "Giống với lan truyền xuôi, chúng ta sẽ triển khai các hàm trợ giúp cho lan truyền ngược. Hãy nhớ rằng lan truyền ngược được sử dụng để tính toán gradient của hàm mất mát liên quan đến các tham số.\n",
    "\n",
    "**Nhắc nhở**: \n",
    "<img src=\"images/backprop_kiank.png\" style=\"width:650px;height:250px;\">\n",
    "<caption><center> **Hình 3** : Lan truyền xuôi và lan truyền ngược cho *LINEAR->RELU->LINEAR->SIGMOID* <br> *Các khối màu tím đại diện cho lan truyền xuôi và các khối màu đỏ đại diện cho lan truyền ngược.*  </center></caption>\n",
    "\n",
    "<!-- \n",
    "For those of you who are expert in calculus (you don't need to be to do this assignment), the chain rule of calculus can be used to derive the derivative of the loss $\\mathcal{L}$ with respect to $z^{[1]}$ in a 2-layer network as follows:\n",
    "\n",
    "$$\\frac{d \\mathcal{L}(a^{[2]},y)}{{dz^{[1]}}} = \\frac{d\\mathcal{L}(a^{[2]},y)}{{da^{[2]}}}\\frac{{da^{[2]}}}{{dz^{[2]}}}\\frac{{dz^{[2]}}}{{da^{[1]}}}\\frac{{da^{[1]}}}{{dz^{[1]}}} \\tag{8} $$\n",
    "\n",
    "In order to calculate the gradient $dW^{[1]} = \\frac{\\partial L}{\\partial W^{[1]}}$, you use the previous chain rule and you do $dW^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial W^{[1]}}$. During the backpropagation, at each step you multiply your current gradient by the gradient corresponding to the specific layer to get the gradient you wanted.\n",
    "\n",
    "Equivalently, in order to calculate the gradient $db^{[1]} = \\frac{\\partial L}{\\partial b^{[1]}}$, you use the previous chain rule and you do $db^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial b^{[1]}}$.\n",
    "\n",
    "This is why we talk about **backpropagation**.\n",
    "!-->\n",
    "\n",
    "Bây giờ, tương tự như lan truyền xuôi, chúng ta sẽ xây dựng lan truyền ngược theo 3 bước::\n",
    "- LINEAR ngược\n",
    "- LINEAR -> ACTIVATION ngược nơi ACTIVATION tính đạo hàm của kích hoạt ReLU hoặc sigmoid\n",
    "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID ngược (toàn bộ mô hình)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46y7WhYUvbSv"
   },
   "source": [
    "### 6.1 - Truyền ngược tuyến tính (Linear backward)\n",
    "\n",
    "Đối với lớp $l$, phần tuyến tính là: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (tiếp theo là kích hoạt).\n",
    "\n",
    "Giả sử bạn đã tính đạo hàm $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. Bạn muốn nhận $(dW^{[l]}, db^{[l]} dA^{[l-1]})$.\n",
    "\n",
    "<img src=\"images/linearback_kiank.png\" style=\"width:250px;height:300px;\">\n",
    "<caption><center> **Figure 4** </center></caption>\n",
    "\n",
    "Ba đầu ra $(dW^{[l]}, db^{[l]}, dA^{[l]})$ được tính bằng đầu vào $dZ^{[l]}$. Đây là công thức bạn cần:\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ZEd7rCuvbSv"
   },
   "source": [
    "**Task 7**: Sử dụng 3 công thức ở trên để thực hiện linear_backward()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_UY9qU_vbSv"
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Triển khai phần tuyến tính của lan truyền ngược cho một lớp duy nhất (lớp l)\n",
    "\n",
    "    Đối số:\n",
    "    dZ -- Gradient của chi phí liên quan tới đầu ra tuyến tính (của lớp l hiện tại)\n",
    "    cache -- tuple của các giá trị (A_prev, W, b) từ lan truyền xuôi ở lớp hiện tại\n",
    "\n",
    "    Trả về:\n",
    "    dA_prev -- Gradient của chi phí liên quan tới kích hoạt (của lớp l-1 trước), có cùng shape với A_prev\n",
    "    dW -- Gradient của chi phí liên quan tới W (current layer l), có cùng shape với W\n",
    "    db -- Gradient của chi phí liên quan tới b (current layer l), có cùng shape với b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### BẮT ĐẦU CODE Ở ĐÂY ### (≈ 3 dòng code)\n",
    "    dW = (1/m)*np.dot(dZ,np.transpose(A_prev))\n",
    "    db = (1/m)*np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_prev = np.dot(np.transpose(W),dZ)\n",
    "    ### KẾT THÚC CODE Ở ĐÂY ###\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hgnyCXd6vbSv",
    "outputId": "735c8bf1-ac3c-46bf-a04b-14942abab06a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = [[ 0.51822968 -0.19517421]\n",
      " [-0.40506361  0.15255393]\n",
      " [ 2.37496825 -0.89445391]]\n",
      "dW = [[-0.10076895  1.40685096  1.64992505]]\n",
      "db = [[0.50629448]]\n"
     ]
    }
   ],
   "source": [
    "# Thiết lập một số đầu vào kiểm tra\n",
    "dZ, linear_cache = linear_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvXL6JkWvbSw"
   },
   "source": [
    "**Kỳ vọng đầu ra**: \n",
    "\n",
    "<table style=\"width:90%\">\n",
    "  <tr>\n",
    "    <td> **dA_prev** </td>\n",
    "    <td > [[ 0.51822968 -0.19517421]\n",
    " [-0.40506361  0.15255393]\n",
    " [ 2.37496825 -0.89445391]] </td> \n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "        <td> **dW** </td>\n",
    "        <td > [[-0.10076895  1.40685096  1.64992505]] </td> \n",
    "    </tr> \n",
    "  \n",
    "    <tr>\n",
    "        <td> **db** </td>\n",
    "        <td> [[ 0.50629448]] </td> \n",
    "    </tr> \n",
    "    \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqOowYtVvbSw"
   },
   "source": [
    "### 6.2 - Truyền ngược kích hoạt tuyến tính (Linear-Activation backward)\n",
    "\n",
    "Tiếp theo, chúng ta sẽ tạo một hàm kết hợp 2 hàm trợ giúp: **`linear_backward`** và bước truyền ngược để kích hoạt **`linear_activation_backward`**.\n",
    "\n",
    "Để giúp bạn triển khai `linear_activation_backward`, chúng tôi đã cung cấp 2 hàm truyền ngược:\n",
    "- **`sigmoid_backward`**: Thực hiện truyền ngược cho đơn vị SIGMOID. Bạn có thể gọi nó như sau: \n",
    "\n",
    "```python\n",
    "dZ = sigmoid_backward(dA, activation_cache)\n",
    "```\n",
    "\n",
    "- **`relu_backward`**: Thực hiện truyền ngược cho đơn vị RELU. Bạn có thể gọi nó như sau:\n",
    "\n",
    "```python\n",
    "dZ = relu_backward(dA, activation_cache)\n",
    "```\n",
    "\n",
    "Nếu $g(.)$ là hàm kích hoạt, \n",
    "`sigmoid_backward` và `relu_backward` sẽ tính $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$.  \n",
    "\n",
    "**Task 8**: Thực hiện lan truyền ngược cho lớp *LINEAR->ACTIVATION*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dmu5seecvbSw"
   },
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Triển khai lan truyền ngược cho lớp LINEAR->ACTIVATION.\n",
    "    \n",
    "    Đối số:\n",
    "    dA -- gradient hậu kích hoạt cho lớp l hiện tại\n",
    "    cache -- tuple của các giá trị (linear_cache, activation_cache) mà chúng ta lưu trữ để tính toán lan truyền ngược một cách hiệu quả\n",
    "    activation -- kích hoạt dùng trong lớp này được lưu trữ thành xâu văn bản: \"sigmoid\" hoặc \"relu\"\n",
    "    \n",
    "    Trả về:\n",
    "    dA_prev -- Gradient của chi phí liên quan tới kích hoạt (của lớp l-1 trước), có cùng shape với A_prev\n",
    "    dW -- Gradient của chi phí liên quan tới W (current layer l), có cùng shape với W\n",
    "    db -- Gradient của chi phí liên quan tới b (current layer l), có cùng shape với b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        ### BẮT ĐẦU CODE Ở ĐÂY ### (≈ 2 dòng code)\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        ### KẾT THÚC CODE Ở ĐÂY ###\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        ### BẮT ĐẦU CODE Ở ĐÂY ### (≈ 2 dòng code)\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        ### KẾT THÚC CODE Ở ĐÂY ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5mMXaD8yvbSw",
    "outputId": "7ec55a4f-0499-47bc-840a-a610331a2f9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[ 0.11017994  0.01105339]\n",
      " [ 0.09466817  0.00949723]\n",
      " [-0.05743092 -0.00576154]]\n",
      "dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
      "db = [[-0.05729622]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[ 0.44090989 -0.        ]\n",
      " [ 0.37883606 -0.        ]\n",
      " [-0.2298228   0.        ]]\n",
      "dW = [[ 0.44513824  0.37371418 -0.10478989]]\n",
      "db = [[-0.20837892]]\n"
     ]
    }
   ],
   "source": [
    "AL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjRsqbdxvbSx"
   },
   "source": [
    "**Kỳ vọng đầu ra với sigmoid:**\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td > dA_prev </td> \n",
    "           <td >[[ 0.11017994  0.01105339]\n",
    " [ 0.09466817  0.00949723]\n",
    " [-0.05743092 -0.00576154]] </td> \n",
    "\n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > dW </td> \n",
    "           <td > [[ 0.10266786  0.09778551 -0.01968084]] </td> \n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > db </td> \n",
    "           <td > [[-0.05729622]] </td> \n",
    "  </tr> \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6b-XLgsQvbSx"
   },
   "source": [
    "**Kỳ vọng đầu ra với relu:**\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td > dA_prev </td> \n",
    "           <td > [[ 0.44090989  0.        ]\n",
    " [ 0.37883606  0.        ]\n",
    " [-0.2298228   0.        ]] </td> \n",
    "\n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > dW </td> \n",
    "           <td > [[ 0.44513824  0.37371418 -0.10478989]] </td> \n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > db </td> \n",
    "           <td > [[-0.20837892]] </td> \n",
    "  </tr> \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUZ-Wyq6vbSx"
   },
   "source": [
    "### 6.3 - Truyền ngược L-mô hình\n",
    "\n",
    "Bây giờ chúng ta sẽ thực hiện hàm truyền ngược cho toàn mạng. Nhớ lại rằng khi triển khai hàm `L_model_osystem`, ở mỗi lần lặp lại, chúng ta đã lưu trữ một cache chứa (X, W, b và z). Trong mô-đun lan truyền ngược, chúng ta sẽ sử dụng các biến đó để tính gradient. Do đó, trong hàm `L_model_backward`, chúng ta sẽ lặp lại tất cả các lớp ẩn truyền ngược, bắt đầu từ lớp $ L $. Ở mỗi bước, chúng ta sẽ sử dụng các giá trị được lưu trong bộ nhớ cache cho lớp $ l $ để sao chép thông qua lớp $ l $. Hình 5 dưới đây cho thấy tính ngược (Backward pass).\n",
    "\n",
    "\n",
    "<img src=\"images/mn_backward.png\" style=\"width:450px;height:300px;\">\n",
    "<caption><center>  **Hình 5** : Backward pass  </center></caption>\n",
    "\n",
    "** Khởi tạo truyền ngược**:\n",
    "Để truyền ngược thông qua mạng này, chúng ta biết đầu ra là\n",
    "$A^{[L]} = \\sigma(Z^{[L]})$. Do đó code của bạn cần tính `dAL` $= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$.\n",
    "Để làm như vậy, hãy sử dụng công thức này (được suy ra bằng phép tính mà bạn không cần kiến ​​thức chuyên sâu):\n",
    "```python\n",
    "dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # đạo hàm của chi phí với AL\n",
    "```\n",
    "\n",
    "Sau đó, chúng ta có thể sử dụng gradient sau kích hoạt `dAL` này để tiếp tục truyền ngược. Như trong Hình 5, giờ chúng ta có thể nhập `dAL` vào hàm truyền ngược LINEAR-> SIGMOID mà các bạn đã triển khai (sẽ sử dụng các giá trị được lưu trong bộ nhớ cache được lưu trữ bởi hàm L_model_osystem). Sau đó, chúng ta sẽ phải sử dụng vòng lặp `for` để lặp qua tất cả các lớp khác bằng cách sử dụng hàm lùi LINEAR->RELU. Nên lưu trữ từng dA, dW và db trong dictionary grads. Để làm như vậy, hãy sử dụng công thức sau:\n",
    "\n",
    "$$grads[\"dW\" + str(l)] = dW^{[l]}\\tag{15} $$\n",
    "\n",
    "Ví dụ: đối với $l=3$ điều này sẽ lưu trữ $dW^{[l]}$ trong `grads[\"dW3\"]`.\n",
    "\n",
    "**Task 9**: Thực hiện truyền ngược cho mô hình *[LINEAR->RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pnB6kEI2vbSx"
   },
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Triển khai lan truyền ngược cho nhóm [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID \n",
    "    \n",
    "    Đối số:\n",
    "    AL -- vectơ xác suất, đầu ra của lan truyền ngược (L_model_forward())\n",
    "    Y -- vectơ true \"label\" (chứa 0 nếu không phải mèo, 1 nếu là mèo)\n",
    "    caches -- danh sách caches chứa:\n",
    "                mọi cache của linear_activation_forward() với \"relu\" (là caches[l], cho l trong range(L-1) chẳng hạn: l = 0...L-2)\n",
    "                cache của linear_activation_forward() với \"sigmoid\" (là caches[L-1])\n",
    "    \n",
    "    Trả về:\n",
    "    grads -- dictionary với gradient\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # số lớp\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # sau dòng này, Y có cùng shape với AL\n",
    "    \n",
    "    # Khởi tạo truyền ngược\n",
    "    ### BẮT ĐẦU CODE Ở ĐÂY ### (1 dòng code)\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # đạo hàm của chi phí với AL\n",
    "    ### KẾT THÚC CODE Ở ĐÂY ###\n",
    "    \n",
    "    # Gradient lớp thứ l (SIGMOID -> LINEAR). Đầu vào: \"AL, Y, caches\". Đầu ra: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### BẮT ĐẦU CODE Ở ĐÂY ### (khoảng 2 dòng)\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "    ### KẾT THÚC CODE Ở ĐÂY ###\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lớp thứ l: (RELU -> LINEAR) gradient.\n",
    "        # Đầu vào: \"grads[\"dA\" + str(l + 2)], caches\". Đầu ra: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### BẮT ĐẦU CODE Ở ĐÂY ### (khoảng 5 dòng)\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, \"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        ### KẾT THÚC CODE Ở ĐÂY ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9i6nALzZvbSy",
    "outputId": "44ce4218-fad5-4410-ed3b-41c904f2f477"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n",
      "db1 = [[-0.22007063]\n",
      " [ 0.        ]\n",
      " [-0.02835349]]\n",
      "dA1 = [[ 0.12913162 -0.44014127]\n",
      " [-0.14175655  0.48317296]\n",
      " [ 0.01663708 -0.05670698]]\n"
     ]
    }
   ],
   "source": [
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "grads = L_model_backward(AL, Y_assess, caches)\n",
    "print_grads(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TN9XZwZvbSy"
   },
   "source": [
    "**Kỳ vọng đầu ra**\n",
    "\n",
    "<table style=\"width:60%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td > dW1 </td> \n",
    "           <td > [[ 0.41010002  0.07807203  0.13798444  0.10502167]\n",
    " [ 0.          0.          0.          0.        ]\n",
    " [ 0.05283652  0.01005865  0.01777766  0.0135308 ]] </td> \n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > db1 </td> \n",
    "           <td > [[-0.22007063]\n",
    " [ 0.        ]\n",
    " [-0.02835349]] </td> \n",
    "  </tr> \n",
    "  \n",
    "  <tr>\n",
    "  <td > dA1 </td> \n",
    "           <td > [[ 0.12913162 -0.44014127]\n",
    " [-0.14175655  0.48317296]\n",
    " [ 0.01663708 -0.05670698]] </td> \n",
    "\n",
    "  </tr> \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJcm32CNvbSy"
   },
   "source": [
    "### 6.4 - Cập nhật Tham số\n",
    "\n",
    "Trong phần này, chúng ta sẽ cập nhật các tham số của mô hình, sử dụng gradient descent:\n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n",
    "\n",
    "trong đó $\\alpha$ là learning rate. Sau khi tính toán các tham số đã cập nhật, hãy lưu trữ chúng trong dictionary tham số."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1mpRyNTvbSy"
   },
   "source": [
    "**Task 10**: Triển khai `update_parameters()` để cập nhật các tham số của bạn sử dụng gradient descent.\n",
    "\n",
    "**Hướng dẫn**:\n",
    "Cập nhật các tham số bằng cách sử dụng gradient descent trên mỗi $W^{[l]}$ và $b^{[l]}$ cho $l = 1, 2, ..., L$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJNMI_I_vbSz"
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Cập nhật tham số sử dụng gradient descent\n",
    "    \n",
    "    Đối số:\n",
    "    parameters -- dictionary của python chứa các tham số \n",
    "    grads -- dictionary của python chứa gradient, đầu ra của L_model_backward\n",
    "    \n",
    "    Trả về:\n",
    "    parameters -- dictionary của python chứa các tham số đã cập nhật \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # số lớp trong mạng nơ-ron\n",
    "\n",
    "    # Quy tắc cập nhật cho từng tham số. Sử dụng vòng lặp for.\n",
    "    ### BẮT ĐẦU CODE Ở ĐÂY ### (≈ 3 dòng code)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] =parameters[\"W\" + str(l+1)]-learning_rate*grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] =parameters[\"b\" + str(l+1)]-learning_rate*grads[\"db\" + str(l+1)]\n",
    "    ### KẾT THÚC CODE Ở ĐÂY ###\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FkPu51GdvbSz",
    "outputId": "bd6856da-2cc2-4670-d791-b5cfd47e9c42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
      " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
      " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
      "b1 = [[-0.04659241]\n",
      " [-1.28888275]\n",
      " [ 0.53405496]]\n",
      "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
      "b2 = [[-0.84610769]]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ay3D5ksdvbS0"
   },
   "source": [
    "**Kỳ vọng đầu ra**:\n",
    "\n",
    "<table style=\"width:100%\"> \n",
    "    <tr>\n",
    "    <td > W1 </td> \n",
    "           <td > [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
    " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
    " [-1.0535704  -0.86128581  0.68284052  2.20374577]] </td> \n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > b1 </td> \n",
    "           <td > [[-0.04659241]\n",
    " [-1.28888275]\n",
    " [ 0.53405496]] </td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td > W2 </td> \n",
    "           <td > [[-0.55569196  0.0354055   1.32964895]]</td> \n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > b2 </td> \n",
    "           <td > [[-0.84610769]] </td> \n",
    "  </tr> \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOt_6vCUvbS0"
   },
   "source": [
    "\n",
    "## 7 - Tổng kết\n",
    "\n",
    "Chúc mừng bạn đã triển khai tất cả các hàm cần thiết để xây dựng một mạng nơ-ron sâu!\n",
    "\n",
    "Chúng tôi biết đây là một lab khá dài nhưng về sau mọi thứ sẽ tốt hơn. Phần tiếp theo của lab sẽ dễ dàng hơn.\n",
    "\n",
    "Trong lab tiếp theo, bạn sẽ kết hợp tất cả những thứ này lại với nhau để tạo ra 2 mô hình:\n",
    "- Mạng nơ-ron 2 lớp\n",
    "- Mạng nơ-ron L lớp\n",
    "\n",
    "Trên thực tế, bạn sẽ sử dụng những mô hình này để phân loại hình ảnh là mèo và không phải mèo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "DVpg-mLGvbS1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[VN]Lab_4_1_DNN.ipynb",
   "provenance": []
  },
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "c4HO0",
   "launcher_item_id": "lSYZM"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
